{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f8979fc",
   "metadata": {},
   "source": [
    "# LoRA finetuning for Qwen 2.5 14B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b92a3b3",
   "metadata": {},
   "source": [
    "## Start with setup of environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a3f6161",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PreTrainedModel' from 'transformers' (/home/jovyan/.local/lib/python3.10/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     AutoTokenizer, \n\u001b[1;32m      5\u001b[0m     AutoModelForCausalLM,\n\u001b[1;32m      6\u001b[0m     TrainingArguments,\n\u001b[1;32m      7\u001b[0m     Trainer,\n\u001b[1;32m      8\u001b[0m     DataCollatorForLanguageModeling,\n\u001b[1;32m      9\u001b[0m     BitsAndBytesConfig\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtrl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SFTConfig, SFTTrainer\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2317\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2315\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module:\n\u001b[1;32m   2316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2317\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2318\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   2319\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2347\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   2346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2345\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   2344\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2345\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2346\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/usr/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:42\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any, Callable, Optional, Union\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Integrations must be imported before ML frameworks:\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# ruff: isort: off\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     43\u001b[0m     get_reporting_integration_callbacks,\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# ruff: isort: on\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhf_hub_utils\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2317\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2315\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module:\n\u001b[1;32m   2316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2317\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2318\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   2319\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2347\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   2346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2345\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   2344\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2345\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2346\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/usr/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/integrations/integration_utils.py:44\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWANDB_MODE\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffline\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚙️  Running in WANDB offline mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel, TrainingArguments\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m version\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     47\u001b[0m     PushToHubMixin,\n\u001b[1;32m     48\u001b[0m     flatten_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m     logging,\n\u001b[1;32m     54\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'PreTrainedModel' from 'transformers' (/home/jovyan/.local/lib/python3.10/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "import gc\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb759e54",
   "metadata": {},
   "source": [
    "## For Logging purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c4fe343c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mt-p-angevare\u001b[0m (\u001b[33mt-p-angevare-university-of-twente\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20260115_225319-oubxpx5f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning/runs/oubxpx5f' target=\"_blank\">14b-ioc-extraction</a></strong> to <a href='https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning' target=\"_blank\">https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning/runs/oubxpx5f' target=\"_blank\">https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning/runs/oubxpx5f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning/runs/oubxpx5f?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7faf521b3c40>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"Qwen-fine-tuning\", name=\"14b-ioc-extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "360cf854",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842bd301",
   "metadata": {},
   "source": [
    "## Usage of AI4privacy dataset cleaning and implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18387680",
   "metadata": {},
   "source": [
    "https://huggingface.co/datasets/ai4privacy/pii-masking-300k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "97352ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source_text', 'privacy_mask', 'id'],\n",
       "        num_rows: 29908\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['source_text', 'privacy_mask', 'id'],\n",
       "        num_rows: 7946\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"ai4privacy/pii-masking-300k\")\n",
    "dataset = dataset.filter(lambda x: x['language'] == 'English')\n",
    "dataset = dataset.select_columns([\"source_text\", \"privacy_mask\", \"id\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ceb6ee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_entity_mapping = {\n",
    "    'EMAIL' : 'EMAIL',\n",
    "    'LASTNAME1' : 'PERSON',\n",
    "    'IP' : 'IP',\n",
    "    'GIVENNAME1' : 'PERSON',\n",
    "    'TEL' : 'PHONE',\n",
    "    'CITY' : 'LOCATION',\n",
    "    'POSTCODE' : 'LOCATION',\n",
    "    'STREET': 'LOCATION',\n",
    "    'STATE' : 'LOCATION',\n",
    "    'BUILDING' : 'LOCATION',\n",
    "    'COUNTRY' : 'LOCATION',\n",
    "    'SECADDRESS' : 'LOCATION',\n",
    "    'LASTNAME2' : 'PERSON',\n",
    "    'GIVENNAME2' : 'PERSON',\n",
    "    'GEOCOORD' : 'LOCATION',\n",
    "    'LASTNAME3' : 'PERSON'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "04564bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_entities(privacy_mask, source_text):\n",
    "    \"\"\"\n",
    "    Clean and combine entities, merging consecutive PERSON entities into full names.\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    for ent in privacy_mask:\n",
    "        if ent['label'] in dataset_entity_mapping.keys():\n",
    "            entities.append({\n",
    "                'type': dataset_entity_mapping.get(ent['label']),\n",
    "                'text': ent['value'],\n",
    "                'start_pos': ent['start'],\n",
    "                'end_pos': ent['end'],\n",
    "                'original_label': ent['label']\n",
    "            })\n",
    "    \n",
    " \n",
    "    entities.sort(key=lambda x: x['start_pos'])\n",
    "\n",
    "    merged_entities = []\n",
    "    i = 0\n",
    "    while i < len(entities):\n",
    "        current = entities[i]\n",
    "        \n",
    "        if current['type'] == 'PERSON' and i + 1 < len(entities):\n",
    "            next_ent = entities[i + 1]\n",
    "            \n",
    "            if (next_ent['type'] == 'PERSON' and \n",
    "                next_ent['start_pos'] - current['end_pos'] <= 3):\n",
    "\n",
    "                is_first_given = 'GIVENNAME' in current['original_label']\n",
    "                is_second_last = 'LASTNAME' in next_ent['original_label']\n",
    "                is_first_last = 'LASTNAME' in current['original_label']\n",
    "                is_second_given = 'GIVENNAME' in next_ent['original_label']\n",
    "                \n",
    "                if (is_first_given and is_second_last) or (is_first_last and is_second_given):\n",
    "                    # Merge into full name\n",
    "                    full_name = source_text[current['start_pos']:next_ent['end_pos']]\n",
    "                    merged_entities.append({\n",
    "                        'type': 'PERSON',\n",
    "                        'text': full_name,\n",
    "                        'start_pos': current['start_pos'],\n",
    "                        'end_pos': next_ent['end_pos']\n",
    "                    })\n",
    "                    i += 2 \n",
    "                    continue\n",
    "        \n",
    "  \n",
    "        merged_entities.append({\n",
    "            'type': current['type'],\n",
    "            'text': current['text'],\n",
    "            'start_pos': current['start_pos'],\n",
    "            'end_pos': current['end_pos']\n",
    "        })\n",
    "        i += 1\n",
    "    \n",
    "    return merged_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "60f4e32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a71f94b063149f4a8aa0bc4e3a1930b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9473b785417941e1922171fecf577f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7946 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: {'privacy_mask': clean_entities(x['privacy_mask'], x['source_text'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0450yvh9az3x",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a980025897f4c81ab6be4fb7460aecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/29908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfcc04fd4c9e4d4a9df4c33444532e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7946 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out empty samples:\n",
      "  Train: 29908 -> 20042 (9866 removed)\n",
      "  Val: 7946 -> 5215 (2731 removed)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "original_train_size = len(dataset['train'])\n",
    "original_val_size = len(dataset['validation'])\n",
    "\n",
    "dataset = dataset.filter(lambda x: len(x['privacy_mask']) > 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81dc3f4",
   "metadata": {},
   "source": [
    "## Set up LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d56659c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are a cyber intelligence analyst with 20 years of experience in the the field.\n",
    "\n",
    "Your task is to extract any entity from the input text. For each entity found you MUST indicate the type in UPPERCASE. ONLY extract entities if literal entity is present in input text.\n",
    "The expected entity types are the following:\n",
    "\n",
    "- EMAIL: email addresses format (user@domain.tld)\n",
    "- IP: IP addresses (IPv4 x.x.x.x or IPv6)\n",
    "- BTC: ONLY Bitcoin wallet addresses (26-35 alphanumeric, starting with 1, 3, or bc1) EXCLUDE the word bitcoin or values (for example 2.0 BTC)\n",
    "- IBAN: iban bank account number\n",
    "- PERSON: Human names (John Smith, John, Catalina) EXCLUDE initials (for example A.H.) \n",
    "- LOCATION: cities, countries, geographic locations, regions\n",
    "- PHONE: phone numbers in any format\n",
    "- URL: URLs and web addresses EXCLUDE filenames\n",
    "- TOX: Tox messenger IDs (76 character hexadecimal strings)\n",
    " \n",
    "**Output**:\n",
    "The output MUST be in a JSON object with key 'entities' and the value a list of dictionaries including every entity found. For each entity you MUST indicate the type in UPPERCASE.\n",
    "\n",
    "**OUTPUT EXAMPLE**:\n",
    "{\n",
    "  \"entities\": [\n",
    "    {\"entity\": \"<extracted text>\", \"type\": \"<TYPE>\"},\n",
    "    {\"entity\": \"<extracted text>\", \"type\": \"<TYPE>\"},\n",
    "  ]\n",
    "}\n",
    "\n",
    "Return empty array if no entities found in the input text.\n",
    "PAY ATTENTION to sentences that begin with entity type PERSON, for example Anna.\n",
    "PAY ATTENTION to when the sentences begin with possesive forms of entity type PERSON, for example Catalina's\n",
    "PAY ATTENTION to when the sentences contain a FULL NAME, the FULL NAME MUST be extracted as ONE entity.\n",
    "DO NOT include any entities from the example or the system prompt in your answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "17e45961",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-14B\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8d907eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_to_chatml(source_text, privacy_mask):\n",
    "    # Convert to the JSON format expected by the prompt\n",
    "    entities_json = {\n",
    "        \"entities\": [\n",
    "            {\"entity\": ent['text'], \"type\": ent['type']} \n",
    "            for ent in privacy_mask\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": source_text},\n",
    "        {\"role\": \"assistant\", \"content\": json.dumps(entities_json, indent=2)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b2936c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d182c087c3742e3a9e2209b990f8236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20042 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77df6252a9be4cf7b9dff9a4faf3b898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5215 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "dataset = dataset.map(lambda x: {\"messages\": convert_to_chatml(x['source_text'], x['privacy_mask'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "17bde313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 5000\n",
      "Validation samples: 500\n"
     ]
    }
   ],
   "source": [
    "# Reduced dataset for faster training (~2-3 hours instead of 12+)\n",
    "train = dataset['train'].select(range(5000))  # 5k samples (was 30k)\n",
    "val = dataset['validation'].select(range(500))\n",
    "\n",
    "print(f\"Training samples: {len(train)}\")\n",
    "print(f\"Validation samples: {len(val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9b196bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bf51c16d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d2d4cdbafd4c0a8c5f8c45fd30004d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        use_cache=False\n",
    "    )\n",
    "\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ad7848",
   "metadata": {},
   "source": [
    "alpha = 2*r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "67e195b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                   \n",
    "    lora_alpha=16,           \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dce7b5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42cd6cc6ef8b420d8617915cb04dc739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d1520931f341e9941832099c86bf4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e48798b7cfa4854a60aaab80e96325c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f353b9e9714e088fafdc2f43c48f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration for DeepSeek-R1-Distill-Qwen-14B:\n",
      "  - Train samples: 5000\n",
      "  - Epochs: 3\n",
      "  - Effective batch size: 16\n",
      "  - Learning rate: 1e-05\n",
      "  - Eval every 50 steps\n",
      "  - Max grad norm: 0.5\n",
      "  - Early stopping patience: 2 evals\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./sft_qwen_14b_output\",\n",
    "\n",
    "    num_train_epochs=3,                 \n",
    "    \n",
    "    max_length=512,\n",
    "    per_device_train_batch_size=1,       \n",
    "    gradient_accumulation_steps=16,      \n",
    "    \n",
    "\n",
    "    learning_rate=1e-5,                  \n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=0.5,                   \n",
    "    \n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,                      \n",
    "    \n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "\n",
    "    packing=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"qwen-14b-pii\",\n",
    "    bf16=True,\n",
    "    optim=\"adamw_8bit\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=val,\n",
    "    peft_config=lora_config,\n",
    "    processing_class=tokenizer,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(\n",
    "            early_stopping_patience=2,\n",
    "            early_stopping_threshold=0.005\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60ea7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='135' max='939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [135/939 4:49:50 < 29:12:09, 0.01 it/s, Epoch 0.43/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.414400</td>\n",
       "      <td>2.408647</td>\n",
       "      <td>2.198840</td>\n",
       "      <td>409587.000000</td>\n",
       "      <td>0.515223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.273200</td>\n",
       "      <td>2.255872</td>\n",
       "      <td>2.296701</td>\n",
       "      <td>819176.000000</td>\n",
       "      <td>0.522957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2417ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./sft_qwen_14b_output/final_model\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/entropy</td><td>██▆▃▁▁▁▁▁</td></tr><tr><td>eval/loss</td><td>█▇▆▂▁▁▁▁▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁▁▂▇█████</td></tr><tr><td>eval/num_tokens</td><td>▁▂▃▄▅▅▆▇█</td></tr><tr><td>eval/runtime</td><td>█▄▅▅▆▁▁▁▇</td></tr><tr><td>eval/samples_per_second</td><td>▁▄▄▄▄███▁</td></tr><tr><td>eval/steps_per_second</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/entropy</td><td>████████████▇▇▆▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>+5</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/entropy</td><td>0.6061</td></tr><tr><td>eval/loss</td><td>0.57973</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.88522</td></tr><tr><td>eval/num_tokens</td><td>3642927.0</td></tr><tr><td>eval/runtime</td><td>828.6719</td></tr><tr><td>eval/samples_per_second</td><td>0.603</td></tr><tr><td>eval/steps_per_second</td><td>0.076</td></tr><tr><td>train/entropy</td><td>0.61093</td></tr><tr><td>train/epoch</td><td>1.4384</td></tr><tr><td>train/global_step</td><td>450</td></tr><tr><td>+5</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">14b-ioc-extraction</strong> at: <a href='https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning/runs/pvlhwu6o' target=\"_blank\">https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning/runs/pvlhwu6o</a><br> View project at: <a href='https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning' target=\"_blank\">https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260114_172919-pvlhwu6o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./sft_qwen_14b_output/final_model\")\n",
    "print(\"Model saved to ./sft_qwen_14b_output/final_model\")\n",
    "\n",
    "# Log final metrics\n",
    "if wandb.run:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "bd00623c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    eval/entropy  eval/samples_per_second  _step  train/epoch  train/entropy  \\\n",
      "0            NaN                      NaN      0       0.0320       1.958536   \n",
      "1            NaN                      NaN      1       0.0640       1.958171   \n",
      "2            NaN                      NaN      2       0.0960       1.957968   \n",
      "3            NaN                      NaN      3       0.1280       1.957781   \n",
      "4            NaN                      NaN      4       0.1600       1.956328   \n",
      "5       1.958243                    0.606      5       0.1600            NaN   \n",
      "6            NaN                      NaN      6       0.1920       1.963717   \n",
      "7            NaN                      NaN      7       0.2240       2.016932   \n",
      "8            NaN                      NaN      8       0.2560       2.084045   \n",
      "9            NaN                      NaN      9       0.2880       2.094191   \n",
      "10           NaN                      NaN     10       0.3200       2.071899   \n",
      "11      2.058485                    0.610     11       0.3200            NaN   \n",
      "12           NaN                      NaN     12       0.3520       2.041616   \n",
      "13           NaN                      NaN     13       0.3840       1.995839   \n",
      "14           NaN                      NaN     14       0.4160       1.924913   \n",
      "15           NaN                      NaN     15       0.4480       1.788786   \n",
      "16           NaN                      NaN     16       0.4800       1.630296   \n",
      "17      1.553811                    0.610     17       0.4800            NaN   \n",
      "18           NaN                      NaN     18       0.5120       1.482840   \n",
      "19           NaN                      NaN     19       0.5440       1.313624   \n",
      "20           NaN                      NaN     20       0.5760       1.144133   \n",
      "21           NaN                      NaN     21       0.6080       0.954084   \n",
      "22           NaN                      NaN     22       0.6400       0.733966   \n",
      "23      0.602679                    0.610     23       0.6400            NaN   \n",
      "24           NaN                      NaN     24       0.6720       0.519782   \n",
      "25           NaN                      NaN     25       0.7040       0.342780   \n",
      "26           NaN                      NaN     26       0.7360       0.181405   \n",
      "27           NaN                      NaN     27       0.7680       0.081144   \n",
      "28           NaN                      NaN     28       0.8000       0.042711   \n",
      "29      0.034833                    0.609     29       0.8000            NaN   \n",
      "30           NaN                      NaN     30       0.8320       0.032575   \n",
      "31           NaN                      NaN     31       0.8640       0.029895   \n",
      "32           NaN                      NaN     32       0.8960       0.028767   \n",
      "33           NaN                      NaN     33       0.9280       0.028282   \n",
      "34           NaN                      NaN     34       0.9600       0.027889   \n",
      "35      0.027035                    0.605     35       0.9600            NaN   \n",
      "36           NaN                      NaN     36       0.9920       0.027752   \n",
      "37           NaN                      NaN     37       1.0224       0.027470   \n",
      "38           NaN                      NaN     38       1.0544       0.027178   \n",
      "39           NaN                      NaN     39       1.0864       0.027190   \n",
      "40           NaN                      NaN     40       1.1184       0.027209   \n",
      "41      0.026362                    0.604     41       1.1184            NaN   \n",
      "42           NaN                      NaN     42       1.1184            NaN   \n",
      "\n",
      "    eval/steps_per_second  train/loss    _runtime  eval/runtime  \\\n",
      "0                     NaN      2.1611  786.563118           NaN   \n",
      "1                     NaN      2.1598  786.563118           NaN   \n",
      "2                     NaN      2.1558  786.563118           NaN   \n",
      "3                     NaN      2.1475  786.563118           NaN   \n",
      "4                     NaN      2.1315  786.563118           NaN   \n",
      "5                   0.076         NaN  786.563118      824.5038   \n",
      "6                     NaN      2.1021  786.563118           NaN   \n",
      "7                     NaN      2.0671  786.563118           NaN   \n",
      "8                     NaN      2.0409  786.563118           NaN   \n",
      "9                     NaN      2.0135  786.563118           NaN   \n",
      "10                    NaN      1.9718  786.563118           NaN   \n",
      "11                  0.077         NaN  786.563118      819.3431   \n",
      "12                    NaN      1.9188  786.563118           NaN   \n",
      "13                    NaN      1.8527  786.563118           NaN   \n",
      "14                    NaN      1.7712  786.563118           NaN   \n",
      "15                    NaN      1.6675  786.563118           NaN   \n",
      "16                    NaN      1.5434  786.563118           NaN   \n",
      "17                  0.077         NaN  786.563118      820.0341   \n",
      "18                    NaN      1.3989  786.563118           NaN   \n",
      "19                    NaN      1.2289  786.563118           NaN   \n",
      "20                    NaN      1.0360  786.563118           NaN   \n",
      "21                    NaN      0.8215  786.563118           NaN   \n",
      "22                    NaN      0.5826  786.563118           NaN   \n",
      "23                  0.077         NaN  786.563118      820.2847   \n",
      "24                    NaN      0.3598  786.563118           NaN   \n",
      "25                    NaN      0.1819  786.563118           NaN   \n",
      "26                    NaN      0.0666  786.563118           NaN   \n",
      "27                    NaN      0.0185  786.563118           NaN   \n",
      "28                    NaN      0.0065  786.563118           NaN   \n",
      "29                  0.077         NaN  786.563118      820.5675   \n",
      "30                    NaN      0.0045  786.563118           NaN   \n",
      "31                    NaN      0.0041  786.563118           NaN   \n",
      "32                    NaN      0.0039  786.563118           NaN   \n",
      "33                    NaN      0.0038  786.563118           NaN   \n",
      "34                    NaN      0.0037  786.563118           NaN   \n",
      "35                  0.076         NaN  786.563118      827.0299   \n",
      "36                    NaN      0.0037  786.563118           NaN   \n",
      "37                    NaN      0.0037  786.563118           NaN   \n",
      "38                    NaN      0.0036  786.563118           NaN   \n",
      "39                    NaN      0.0036  786.563118           NaN   \n",
      "40                    NaN      0.0036  786.563118           NaN   \n",
      "41                  0.076         NaN  786.563118      827.7177   \n",
      "42                    NaN         NaN  786.563118           NaN   \n",
      "\n",
      "    train/grad_norm  train/learning_rate  eval/loss    _timestamp  \\\n",
      "0          0.960938         9.574468e-07        NaN  1.768229e+09   \n",
      "1          0.972656         2.021277e-06        NaN  1.768230e+09   \n",
      "2          0.957031         3.085106e-06        NaN  1.768231e+09   \n",
      "3          0.976562         4.148936e-06        NaN  1.768232e+09   \n",
      "4          0.960938         5.212766e-06        NaN  1.768234e+09   \n",
      "5               NaN                  NaN   2.115630  1.768234e+09   \n",
      "6          0.789062         6.276596e-06        NaN  1.768236e+09   \n",
      "7          0.349609         7.340426e-06        NaN  1.768237e+09   \n",
      "8          0.287109         8.404255e-06        NaN  1.768238e+09   \n",
      "9          0.283203         9.468085e-06        NaN  1.768239e+09   \n",
      "10         0.300781         9.999136e-06        NaN  1.768241e+09   \n",
      "11              NaN                  NaN   1.944338  1.768241e+09   \n",
      "12         0.333984         9.992227e-06        NaN  1.768243e+09   \n",
      "13         0.388672         9.978418e-06        NaN  1.768244e+09   \n",
      "14         0.468750         9.957728e-06        NaN  1.768245e+09   \n",
      "15         0.558594         9.930187e-06        NaN  1.768246e+09   \n",
      "16         0.656250         9.895831e-06        NaN  1.768247e+09   \n",
      "17              NaN                  NaN   1.468676  1.768248e+09   \n",
      "18         0.808594         9.854709e-06        NaN  1.768249e+09   \n",
      "19         0.945312         9.806877e-06        NaN  1.768251e+09   \n",
      "20         1.101562         9.752402e-06        NaN  1.768252e+09   \n",
      "21         1.328125         9.691359e-06        NaN  1.768253e+09   \n",
      "22         1.320312         9.623831e-06        NaN  1.768254e+09   \n",
      "23              NaN                  NaN   0.451372  1.768255e+09   \n",
      "24         1.187500         9.549913e-06        NaN  1.768256e+09   \n",
      "25         0.824219         9.469707e-06        NaN  1.768257e+09   \n",
      "26         0.455078         9.383323e-06        NaN  1.768258e+09   \n",
      "27         0.179688         9.290881e-06        NaN  1.768260e+09   \n",
      "28         0.056641         9.192509e-06        NaN  1.768261e+09   \n",
      "29              NaN                  NaN   0.004920  1.768262e+09   \n",
      "30         0.038574         9.088342e-06        NaN  1.768263e+09   \n",
      "31         0.028564         8.978525e-06        NaN  1.768264e+09   \n",
      "32         0.024780         8.863209e-06        NaN  1.768265e+09   \n",
      "33         0.024902         8.742554e-06        NaN  1.768266e+09   \n",
      "34         0.023438         8.616726e-06        NaN  1.768268e+09   \n",
      "35              NaN                  NaN   0.003593  1.768268e+09   \n",
      "36         0.021484         8.485900e-06        NaN  1.768270e+09   \n",
      "37         0.020996         8.350255e-06        NaN  1.768271e+09   \n",
      "38         0.020630         8.209981e-06        NaN  1.768272e+09   \n",
      "39         0.020752         8.065270e-06        NaN  1.768273e+09   \n",
      "40         0.019409         7.916322e-06        NaN  1.768274e+09   \n",
      "41              NaN                  NaN   0.003484  1.768275e+09   \n",
      "42              NaN                  NaN        NaN  1.768275e+09   \n",
      "\n",
      "    train/mean_token_accuracy  train/num_tokens  train/global_step  \\\n",
      "0                    0.569618           81920.0                 10   \n",
      "1                    0.568897          163840.0                 20   \n",
      "2                    0.569313          245760.0                 30   \n",
      "3                    0.569129          327680.0                 40   \n",
      "4                    0.572187          409600.0                 50   \n",
      "5                         NaN               NaN                 50   \n",
      "6                    0.573716          491520.0                 60   \n",
      "7                    0.573141          573440.0                 70   \n",
      "8                    0.571404          655360.0                 80   \n",
      "9                    0.570780          737280.0                 90   \n",
      "10                   0.576492          819200.0                100   \n",
      "11                        NaN               NaN                100   \n",
      "12                   0.583867          901120.0                110   \n",
      "13                   0.595682          983040.0                120   \n",
      "14                   0.608207         1064960.0                130   \n",
      "15                   0.614298         1146880.0                140   \n",
      "16                   0.618334         1228800.0                150   \n",
      "17                        NaN               NaN                150   \n",
      "18                   0.642882         1310720.0                160   \n",
      "19                   0.690607         1392640.0                170   \n",
      "20                   0.745536         1474560.0                180   \n",
      "21                   0.800954         1556480.0                190   \n",
      "22                   0.859283         1638400.0                200   \n",
      "23                        NaN               NaN                200   \n",
      "24                   0.914934         1720320.0                210   \n",
      "25                   0.962133         1802240.0                220   \n",
      "26                   0.987378         1884160.0                230   \n",
      "27                   0.997713         1966080.0                240   \n",
      "28                   1.000000         2048000.0                250   \n",
      "29                        NaN               NaN                250   \n",
      "30                   1.000000         2129920.0                260   \n",
      "31                   1.000000         2211840.0                270   \n",
      "32                   1.000000         2293760.0                280   \n",
      "33                   1.000000         2375680.0                290   \n",
      "34                   1.000000         2457600.0                300   \n",
      "35                        NaN               NaN                300   \n",
      "36                   1.000000         2539520.0                310   \n",
      "37                   1.000000         2617344.0                320   \n",
      "38                   1.000000         2699264.0                330   \n",
      "39                   1.000000         2781184.0                340   \n",
      "40                   1.000000         2863104.0                350   \n",
      "41                        NaN               NaN                350   \n",
      "42                        NaN               NaN                350   \n",
      "\n",
      "    eval/mean_token_accuracy  eval/num_tokens  \n",
      "0                        NaN              NaN  \n",
      "1                        NaN              NaN  \n",
      "2                        NaN              NaN  \n",
      "3                        NaN              NaN  \n",
      "4                        NaN              NaN  \n",
      "5                   0.577299         409600.0  \n",
      "6                        NaN              NaN  \n",
      "7                        NaN              NaN  \n",
      "8                        NaN              NaN  \n",
      "9                        NaN              NaN  \n",
      "10                       NaN              NaN  \n",
      "11                  0.587022         819200.0  \n",
      "12                       NaN              NaN  \n",
      "13                       NaN              NaN  \n",
      "14                       NaN              NaN  \n",
      "15                       NaN              NaN  \n",
      "16                       NaN              NaN  \n",
      "17                  0.628149        1228800.0  \n",
      "18                       NaN              NaN  \n",
      "19                       NaN              NaN  \n",
      "20                       NaN              NaN  \n",
      "21                       NaN              NaN  \n",
      "22                       NaN              NaN  \n",
      "23                  0.892368        1638400.0  \n",
      "24                       NaN              NaN  \n",
      "25                       NaN              NaN  \n",
      "26                       NaN              NaN  \n",
      "27                       NaN              NaN  \n",
      "28                       NaN              NaN  \n",
      "29                  1.000000        2048000.0  \n",
      "30                       NaN              NaN  \n",
      "31                       NaN              NaN  \n",
      "32                       NaN              NaN  \n",
      "33                       NaN              NaN  \n",
      "34                       NaN              NaN  \n",
      "35                  1.000000        2457600.0  \n",
      "36                       NaN              NaN  \n",
      "37                       NaN              NaN  \n",
      "38                       NaN              NaN  \n",
      "39                       NaN              NaN  \n",
      "40                       NaN              NaN  \n",
      "41                  1.000000        2863104.0  \n",
      "42                       NaN              NaN  \n"
     ]
    }
   ],
   "source": [
    "api = wandb.Api()\n",
    "run = api.run(\"/t-p-angevare-university-of-twente/Qwen-fine-tuning/runs/27s35zzj\")\n",
    "history = run.history()\n",
    "print(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
